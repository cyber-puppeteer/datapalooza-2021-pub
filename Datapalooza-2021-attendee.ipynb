{"cells":[{"cell_type":"markdown","source":["Let's load the SF Airbnb dataset and look at its schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4265dc56-a20a-4396-b981-695567b95956"}}},{"cell_type":"code","source":["filePath = \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb.csv\"\n\nrawDF = spark.read.csv(filePath, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n\ndisplay(rawDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33a1b0f5-9b65-455f-a8b7-5918ccaa34ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["rawDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e609bf58-e096-4995-91a7-0a51cfdc5761"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["For the sake of simplicity, we'll only keep certain columns from this dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d43c093-4e9c-4fb3-83d0-582095c9b6fe"}}},{"cell_type":"code","source":["columnsToKeep = [\n  \"host_is_superhost\",\n  \"cancellation_policy\",\n  \"instant_bookable\",\n  \"host_total_listings_count\",\n  \"neighbourhood_cleansed\",\n  \"latitude\",\n  \"longitude\",\n  \"property_type\",\n  \"room_type\",\n  \"accommodates\",\n  \"bathrooms\",\n  \"bedrooms\",\n  \"beds\",\n  \"bed_type\",\n  \"minimum_nights\",\n  \"number_of_reviews\",\n  \"review_scores_rating\",\n  \"review_scores_accuracy\",\n  \"review_scores_cleanliness\",\n  \"review_scores_checkin\",\n  \"review_scores_communication\",\n  \"review_scores_location\",\n  \"review_scores_value\",\n  \"price\"]\n\nbaseDF = rawDF.select(columnsToKeep)\nbaseDF.cache().count()\ndisplay(baseDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b084c4e8-e257-43f2-b245-a59c3248b04e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Fixing Data Types\n\nTake a look at the schema above. You'll notice that the `price` field got picked up as string. For our task, we need it to be a numeric (double type) field. \n\nLet's fix that."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2da9c54e-5479-4fd5-830f-b909e85fb7ab"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, translate\n\nfixedPriceDF = baseDF.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\"))\n\ndisplay(fixedPriceDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6f5b8c3-bb20-40b7-ace5-802dcc854e91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Data Exploration\n\nPadas is a handy tool when it comes to data exploration before actual modeling starts. There are a few caveats whe using pandas:\n\n* If dataset size is too big to fit in memory of a single machine\n* If any data cleanup and transfomation code has to be translate into Spark for prod runs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faceafe5-d35f-466e-945d-6adf7f9b8ffe"}}},{"cell_type":"markdown","source":["## Koalas\nLet's use Koalas to perform Pandas style data exploration operations but in a distributed environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffb20584-74d4-4c69-b0b6-b862b0129fae"}}},{"cell_type":"code","source":["import databricks.koalas as ks\n# Before we start we need to convert Spark dataframe into Koalas dataframe\n# https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.DataFrame.to_koalas.html?highlight=to_koalas\n#\n# ksDF = <Your code goes here>\n\n# Let's try to understand our data by displaying some stats. You'd normally do that using https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html in pandas\n\n# FIXME\n# <Your answer goes here>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32cc6c29-ee29-4cec-b675-96d9a7719071"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Dealing with missing values\n\nIf we look at the count row in the output above, we'll notice that all counts vary. Describe method takes into account only numeric types and not null values. It means we have nulls in the data frame. There are a lot of different ways to handle null values. Sometimes, null can actually be a key indicator of the thing you are trying to predict (e.g. if you don't fill in certain portions of a form, probability of it getting approved decreases).\n\nSome ways to handle nulls:\n* Drop any records that contain nulls\n* Numeric:\n  * Replace them with mean/median/zero/etc.\n* Categorical:\n  * Replace them with the mode\n  * Create a special category for null\n* Use techniques like ALS which are designed to impute missing values\n  \n**If you do ANY imputation techniques for categorical/numerical features, you MUST include an additional field specifying that field was imputed (think about why this is necessary)**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fc1b9db-56d9-4cc1-9991-2ad5c52242f1"}}},{"cell_type":"code","source":["# Let's determine if we have empty values in any of dataframe column. In Pandas you'd probably use the following two methods for this task\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.any.html\n\n# FIXME\n# ksDF = <Your answer here>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"920c0570-c216-42c3-a8d0-b25e0abcfbd2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Impute: Cast to Double\n\nIf pure Spark had been used for data imputation, SparkML's `Imputer` woud've required all fields be of type double [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.Imputer). As an exercise let's cast all integer fields to double."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"866a2adf-8baa-42aa-b273-1049832def37"}}},{"cell_type":"code","source":["# In Spark it this operation would've looked like this\n#\n# from pyspark.sql.functions import col\n# from pyspark.sql.types import IntegerType\n#\n# integerColumns = [x.name for x in baseDF.schema.fields if x.dataType == IntegerType()]\n#\n# for c in integerColumns:\n#   doublesDF = doublesDF.withColumn(c, col(c).cast(\"double\"))\n\nimputeCols = ksDF.columns[ksDF.isna().any().to_pandas()].to_list()\nconversionDict = dict(zip(imputeCols, [\"double\"] * len(imputeCols)))\n\n# API reference\n# https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.astype.html\n\n# FIXME\n# doublesDF = <Your answer here>\n\n# Print types in the DF\ndisplay(doublesDF.dtypes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"477545eb-52a6-48c7-a659-80299bcd1291"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Let's add column with dummy variable for each column where we impute any value. This will help us to trace which values were imputed for further investigations. I.e.\n\n| bedrooms | bedrooms_na |\n|:--------:|:-----------:|\n|    3     |     0       |\n|   null   |     1       |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57e9830d-25d2-463f-8f42-d561244032e2"}}},{"cell_type":"code","source":["# In Spark that would've been looked like this\n\n# from pyspark.sql.functions import when\n\n# imputeCols = [\n#   \"bedrooms\",\n#   \"bathrooms\",\n#   \"beds\", \n#   \"review_scores_rating\",\n#   \"review_scores_accuracy\",\n#   \"review_scores_cleanliness\",\n#   \"review_scores_checkin\",\n#   \"review_scores_communication\",\n#   \"review_scores_location\",\n#   \"review_scores_value\"\n# ]\n\n# for c in imputeCols:\n#   doublesDF = doublesDF.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))\n\n# API Reference\n#\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html\n# https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.astype.html\n\nfor c in imputeCols:\n  # FIXME\n  #doublesDF[c + \"_na\"] = <Your answer here>\n  \ndisplay(doublesDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94531852-05cd-4bcd-b803-26041b1a6683"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Spark version of data imputation\n# \n# from pyspark.ml.feature import Imputer\n# imputer = Imputer(strategy=\"median\", inputCols=imputeCols, outputCols=imputeCols)\n# imputedDF = imputer.fit(doublesDF).transform(doublesDF)\n\n# API References \n# https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.fillna.html\n# https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.median.html\n\n#FIXME\n# imputedDF = doublesDF.<Your answer here>\n\ndisplay(imputedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2492ef78-596f-4079-b9a7-dd766027e027"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Getting rid of extreme values\n\nLet's take a look at the *min* and *max* values of the `price` column:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaf932fe-a511-44f3-b265-ca47ed37c589"}}},{"cell_type":"code","source":["# API reference\n#\n# https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.describe.html\n\n# FIXME\n#display(<Your answer here>)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6043be17-6d58-45cd-9cc2-8003b04332f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["There are some super-expensive listings. We'll assume that those are legit. We can certainly filter the \"free\" Airbnbs though.\n\nLet's see first how many listings we can find where the *price* is zero."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3674724-4152-4aab-83d0-4ca34980f452"}}},{"cell_type":"code","source":["# Spark code\n#\n# imputedDF.filter(col(\"price\") == 0).count()\n\n# API reference \n#\n# https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.count.html\n\n# FIXME\n# print(<Your answer here>)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbcc8c7f-d32f-4382-b9ce-26ba25dc74e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Now only keep rows with a strictly positive *price*."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"736528ce-4698-4fdb-94d5-a9ae84137be4"}}},{"cell_type":"code","source":["# Spark code for reference\n#\n# posPricesDF = imputedDF.filter(col(\"price\") > 0)\n\n# API reference\n#\n# https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html#boolean-indexing\n\n# FIXME\n# posPricesDF = <Your answer here>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cda78fa8-bd89-4ad4-adbd-1d2c6d95d2fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at the *min* and *max* values of the *minimum_nights* column:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a14330a3-3b5e-47f5-9920-4799176f15a8"}}},{"cell_type":"code","source":["display(posPricesDF[\"minimum_nights\"].describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e49cf0ba-70d4-4690-ab09-3c41ad3bb358"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Let's see what how many unique values do we have in 'minimum_nights' column"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0c56fbe-b6cd-43ed-8aef-9ce361fe0b77"}}},{"cell_type":"code","source":["# Spark code\n#\n#display(posPricesDF\n# .groupBy(\"minimum_nights\").count()\n# .orderBy(col(\"count\").desc(), col(\"minimum_nights\"))\n#)\n\n# API reference\n#\n# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html\n\n# FIXME\n#display(<Your answer here>)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d11aec1-3873-4cc3-9f43-66f133807b77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["A minimum stay of one year seems to be a reasonable limit here. Let's filter out those records where the *minimum_nights* is greater then 365:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31055cb0-ff92-42bf-b40c-f219dc60202e"}}},{"cell_type":"code","source":["# Spark code\n# cleanDF = posPricesDF.filter(col(\"minimum_nights\") <= 365)\n\n# API reference\n#\n# https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html#boolean-indexing\n\n# FIXME\n# cleanDF = <Your answer here>\ndisplay(cleanDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52ca431f-f507-4eff-af12-7d8b9856cbc0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["OK, our data is cleansed now. Let's save this DataFrame to a file so that we can start building models with it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c173b55-89bd-4b36-bc0f-1d8a91162322"}}},{"cell_type":"code","source":["outputPath = \"/tmp/datapalooza-2021/sf-airbnb/sf-airbnb-clean.parquet\"\n\ncleanDF.to_spark().write.mode(\"overwrite\").parquet(outputPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"852296a0-a285-4e24-a8f4-efd8c8ed0a6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Modeling\n\nKey advantages of ML on Spark:\n- No need to downsize training dataset\n- Distributed nature of Spark offers performance improvements\n- Same code for data engineering in research and prod application\n\nImportant MLlib concepts:\n- Transformer. Applies rule-based transformations to either prepare data for model training or generate predictions using a trained MLlib model.\n- Estimator. Learns (aka “fits”) parameters from input DataFrame via a .fit() method and returns a Model, which is a transformer.\n- Pipeline. Organizes a series of transformers and estimators into a single model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65fe837e-3f29-4fd9-82a9-fd7909b56a04"}}},{"cell_type":"markdown","source":["## Training and test data sets\n\nLet's use dataset we created during the previous step to derive our training and test data sets. We'll use 80/20 as train/test split and 42 as seed value."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9300de6a-6438-4086-8fca-a0ff6ba97b22"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\n\nairbnbDF = spark.read.parquet(outputPath)\n(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n\ncategoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\nindexOutputCols = [x + \"Index\" for x in categoricalCols]\n\nstringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n\nnumericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n\nassemblerInputs = indexOutputCols + numericCols\n\nvecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"554688d1-814c-4ef4-ae8b-a9a1f780a560"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Random Forest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff91b02e-cece-4561-a64f-e347b78ee175"}}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Please use the following params for RandomForestRegressor: maxDepth=5, numTrees=20, maxBins=40, seed=42\n\n# API reference\n#\n# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html\n\n# FIXME\n# rf = <Your answer here>\n\n# And let's create a pipeline to train the model\n# API reference\n#\n# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html#pyspark.ml.Pipeline\n\n# FIXME\n# pipeline = <Your answer here>\n\n# and train the model \n# API reference\n#\n# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html#pyspark.ml.Pipeline.fit\n\n# FIXME\n# rfModel = <Your answer here>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"421d1326-a47f-4566-98b6-1be3f9cce1cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Let's see how is model's performance"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29a5c5fe-0e9f-443e-95fd-18f5a9068786"}}},{"cell_type":"code","source":["# Let's use our test data to generate some predictions\n# API reference\n#\n# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.PipelineModel.html#pyspark.ml.PipelineModel.transform\n\n# FIXME\n# predictionDF = rfModel.<Your answer here>\ndisplay(predictionDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"672f63a2-f321-498c-8ff7-95f2bcd409c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Let's calculate RMSE for prediction results\n#\n# API reference \n#\n# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html#pyspark.ml.evaluation.RegressionEvaluator\n#\n\nevaluator = RegressionEvaluator(labelCol=\"price\", \n                                predictionCol=\"prediction\", \n                                metricName=\"rmse\")\n\naccuracy = evaluator.evaluate(predictionDF)\n\ndisplay(accuracy)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3323844-cdb0-47fb-a915-b97ec3626b9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Grid Search\n\nThere are a lot of hyperparameters we could tune, and it would take a long time to manually configure.\n\nLet's use Spark's `ParamGridBuilder` to find the optimal hyperparameters in a more systematic approach [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.ParamGridBuilder).\n\nLet's define a grid of hyperparameters to test:\n  - maxDepth: max depth of the decision tree (Use the values `2, 4, 6`)\n  - numTrees: number of decision trees (Use the values `10, 100`)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a8df729-c7fc-44c0-80f6-f65f89209658"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n\nparamGrid = (ParamGridBuilder()\n            .addGrid(rf.maxDepth, [2, 4, 6])\n            .addGrid(rf.numTrees, [10, 100])\n            .build())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7eb8368a-a928-4a16-bf57-b16114420db3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Cross Validation\n\nWe are also going to use 3-fold cross validation to identify the optimal maxDepth.\n\n![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n\nWith 3-fold cross-validation, we train on 2/3 of the data, and evaluate with the remaining (held-out) 1/3. We repeat this process 3 times, so each fold gets the chance to act as the validation set. We then average the results of the three rounds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5e05c96-1ff1-4ea0-9cae-5d1ea0afffe9"}}},{"cell_type":"markdown","source":["We pass in the `estimator` (pipeline), `evaluator`, and `estimatorParamMaps` to `CrossValidator` so that it knows:\n- Which model to use\n- How to evaluate the model\n- What hyperparameters to set for the model\n\nWe can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.CrossValidator)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d1f7a7c-4cf4-4db8-94ad-045a745138fe"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\nevaluator = RegressionEvaluator(labelCol=\"price\", \n                                predictionCol=\"prediction\", \n                                metricName=\"rmse\")\n\ncv = CrossValidator(estimator=pipeline, \n                    evaluator=evaluator, \n                    estimatorParamMaps=paramGrid, \n                    numFolds=3, \n                    seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98d5962d-af6b-4b79-9ea8-d93acf755b2f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["**Question**: How many models are we training right now?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f641978-88b4-46bf-a6fb-51c0dcee4822"}}},{"cell_type":"code","source":["cvModel = cv.fit(trainDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf850093-548d-413c-a375-37d26ee8c0b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Let's take a look at the model with the best hyperparameter configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70e3b6be-e3ea-401b-a01b-afd1194cb46b"}}},{"cell_type":"code","source":["print(cvModel.bestModel.stages[2].getMaxDepth())\nprint(cvModel.bestModel.stages[2].getNumTrees)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"834bc576-78f9-498a-9e6f-7815694cbe15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(cvModel.avgMetrics)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43887fbb-2bd1-4b9e-bfd3-d54069ccc55b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Let's see how it does on the test dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11052d16-69f2-4ab2-a3ff-a9d4b6ac1aa5"}}},{"cell_type":"code","source":["predDF = cvModel.transform(testDF)\n\nregressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n\nrmse = regressionEvaluator.evaluate(predDF)\nr2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\nprint(f\"RMSE is {rmse}\")\nprint(f\"R2 is {r2}\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed640e6a-dac4-4508-83df-05eea6345540"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Tracking Models with MLflow\n\nMLflow is pre-installed on the Databricks Runtime for ML. If you are not using the ML Runtime, you will need to install mlflow. MLflow has four main components:\n\n- Tracking. Provides APIs to record parameters, metrics, code versions, models, and artifacts such as plots, and text.\n- Projects. A standardized format to package your data science projects and their dependencies to run on other platforms. It helps you manage the model training process.\n- Models. A standardized format to package models to deploy to diverse execution environments. It provides a consistent API for loading and applying models, regardless of the algorithm or library used to build the model.\n- Registry. A repository to keep track of model lineage, model versions, stage transitions, and annotations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e6fbd7c-b8dc-4b42-ab36-d9256be14f49"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nfilePath = \"dbfs:/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\nairbnbDF = spark.read.parquet(filePath)\n(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n\ncategoricalCols = [field for (field, dataType) in trainDF.dtypes \n                   if dataType == \"string\"]\nindexOutputCols = [x + \"Index\" for x in categoricalCols]\nstringIndexer = StringIndexer(inputCols=categoricalCols, \n                              outputCols=indexOutputCols, \n                              handleInvalid=\"skip\")\n\nnumericCols = [field for (field, dataType) in trainDF.dtypes \n               if ((dataType == \"double\") & (field != \"price\"))]\nassemblerInputs = indexOutputCols + numericCols\nvecAssembler = VectorAssembler(inputCols=assemblerInputs, \n                               outputCol=\"features\")\n\nrf = RandomForestRegressor(labelCol=\"price\", maxBins=40, maxDepth=5, \n                           numTrees=100, seed=42)\n\npipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2eea1168-6fd2-467a-be42-90a2708245ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## MLflow\n\nBefore you deploy your machine learning model, you should ensure that you can reproduce and track the model’s performance. For us, end-to-end reproducibility of machine learning solutions means that we need to be able to reproduce the code that generated a model, the environment used in training, the data it was trained on, and the model itself. Every data scientist loves to remind you to set your seeds so you can reproduce your experiments (e.g., for the train/test split, when using models with inherent randomness such as random forests)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dfa3262-4866-4d3a-856a-a3ed0eba6667"}}},{"cell_type":"code","source":["import mlflow\nimport mlflow.spark\nimport pandas as pd\n\nwith mlflow.start_run(run_name=\"random-forest\") as run:\n  # Log params: Num Trees and Max Depth\n  mlflow.log_param(\"num_trees\", rf.getNumTrees())\n  mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n \n  # Log model\n  pipelineModel = pipeline.fit(trainDF)\n  mlflow.spark.log_model(pipelineModel, \"model\")\n\n  # Log metrics: RMSE and R2\n  predDF = pipelineModel.transform(testDF)\n  regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n                                            labelCol=\"price\")\n  rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n  r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n  mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n\n  # Log artifact: Feature Importance Scores\n  rfModel = pipelineModel.stages[-1]\n  pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(), \n                                    rfModel.featureImportances)), \n                          columns=[\"feature\", \"importance\"])\n              .sort_values(by=\"importance\", ascending=False))\n  # First write to local filesystem, then tell MLflow where to find that file\n  pandasDF.to_csv(\"feature-importance.csv\", index=False)\n  mlflow.log_artifact(\"feature-importance.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f0c84c3-108d-4ddf-a99a-6d1b86d0f4d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## MLflowClient"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7bf3254-a001-4b56-9b9f-e3edfb4da571"}}},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\nruns = client.search_runs(run.info.experiment_id,\n                          order_by=[\"attributes.start_time desc\"], \n                          max_results=1)\nrun_id = runs[0].info.run_id\nruns[0].data.metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"523f4a0d-3082-4d1d-8f03-1d0ec3a4b216"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["%md ## Generate Batch Predictions\n\nLet's load the model back in to generate batch predictions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"514f6cfc-830c-486f-b151-da40491a1e1d"}}},{"cell_type":"code","source":["# Load saved model with MLflow\npipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n\n# Generate Predictions\ninputPath = \"dbfs:/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\"\ninputDF = spark.read.parquet(inputPath)\npredDF = pipelineModel.transform(inputDF)\ndisplay(predDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aac7ecdd-89c2-4899-a695-cd81955b0827"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Datapalooza-2021-attendee","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1101586441279579}},"nbformat":4,"nbformat_minor":0}
